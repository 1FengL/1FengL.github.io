---
layout: post
title: Word2Vec
---

In NLP, words are the main objects that we are dealing with. How should we represent words in a form that computers can store? A straightforward answer can be let's representing them as discrete symbols:
<center>dog = [0 0 0 0 1 0 0 ... 0 0]</center>
<center>cat = [0 0 1 0 0 0 0 ... 0 0]</center>

which are also called one-hot vectors, where an 1 is in a location unique to the specific word and 0s are elsewhere.

However, one-hot representations have mainly two drawbacks:
* Vectors can be too long that we cannot afford as the dictionary size grows
+ All vectors are orthogonal and thus there is no natural notion of **similarity** in one-hot vectors

Therefore, we would like to represent words in dense vectors that encode similarity to solve above drawbacks. Word vectors, also known as word embeddings or word representations,
![_config.yml]({{ site.baseurl }}/images/config.png)

The easiest way to make your first post is to edit this one. Go into /_posts/ and update the Hello World markdown file. For more instructions head over to the [Jekyll Now repository](https://github.com/barryclark/jekyll-now) on GitHub.
