---
layout: post
title: Word2Vec
---

In NLP, words are the main objects that we deal with. How should we represent words in a form that computers can store? A straightforward answer can be representing them as discrete symbols:
<center>dog = [0 0 0 0 1 0 0 ... 0 0]</center>

<center>cat = [0 0 1 0 0 0 0 ... 0 0]</center>

which are also called one-hot vectors, where an 1 is in a location unique to the specific word and 0s are elsewhere.

However, one-hot representations have mainly two drawbacks:
* Vectors can be too long that we cannot afford as the dictionary size grows
+ All vectors are orthogonal and thus there is no natural notion of **similarity** in one-hot vectors.

Therefore, we would like to represent words in dense vectors that encode similarity to solve above drawbacks. For instance, we may want our word vectors end like:

$$
dog = 
 \left[
 \begin{matrix}
   1.1\\
   1.9\\
   ...\\
   3.9\\
 \end{matrix}
 \right]
cat = 
\left[
 \begin{matrix}
   1.2\\
   2.0\\
   ...\\
   4.5\\
 \end{matrix}
\right]
$$

Due to that dogs&cats are both animals, they have some dimensions sharing similar values. On the other hand, they belong to different species so some other dimensions vary in terms of value.

Word vectors, also known as word2vec, word embeddings or word representations, builds a dense vecotor for each word, and enables that similar meanings also have similar vectors. To do so, we may **represent a word according to words that frequently appear nearby**, for that synonyms can substitue each other and be placed in same context.

## 1. Skip-Gram Model

#### 1.1 Introduction

The Skip-Gram model is one of the frameworks for learning word vectors. Under this framework, we represent each word by a vector in a way that, given a word, we can predict its surrounding words. Namely, the probability:

$$P(w_{t-m}|w_t, \theta)$$

is maximized, where \\(w_{t}\\) is the one-hot vector of the given center word, \\(w_{t-m}\\) represents surrounding word, m is called window size and $$\theta$$ is the parameters of skip-gram model. To achieve this, we use a small window with size m to scan the whole sentence, and keep maximizing the probability.

![_config.yml]({{ site.baseurl }}/images/word2vec1.png)
<center>Fig.1</center>
![_config.yml]({{ site.baseurl }}/images/word2vec2.png)
<center>Fig.2</center>

#### 1.2 Loss function

Mathematically, we multiply all probabilities using a likelihood function to numerically measure how good our model is:

$$ L(\theta) = \prod_{t=1}^{T}\prod_{-m\le j\le m, j\neq0}P(w_{t+j}|w_t,\theta)$$

We can put a log and a minus sign on both sides, to turn product into sum, and turn maximization into minimization, and then take average:

$$ J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T} \sum_{t=1}^{T}\sum_{-m\le j\le m, j\neq0}logP(w_{t+j}|w_t,\theta)$$

and J is the **loss function** that need to be minimized.

Now, the question becomes how do we calculate probability $$ P(w_{t+j}|w_t,\theta) $$ ?
Instead of giving each word one vector, we assign two different vecotrs to every word, one representing the word when it is a center word and the other when it is a context word:

* \\(v_w\\) when \\(w\\) is a center word
* \\(u_w\\) when \\(w\\) is a context word

>1.Constructing two vectors makes it easier to optimize them later. At then end these two vectors are averaged to produce the final vector.
>
>2.Parameters $$\theta$$ are used to transform one-hot vectors $$w$$ into word vector $$v$$ and $$u$$.

Then for a center word \\(c\\) and a context word \\(o\\), the probability becomes:

$$P(o|c, \theta) = \frac{exp(u_o^Tv_c)}{\sum_{w\subseteq V} exp(u^T_w v_c))}$$

where V is the size of dictionary (all words).

>1.First, the dot product of vector of center word $$v_c$$ and context word $$u_w$$ is calculated as a score, measuring how far these two vectors are.
>
>2.Then, a **softmax** function:
>$$ softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n}{exp(x_j)}} = p_i $$
>is applied to calculate probability $$P(o|c)$$.
>
>3.Softmax function maps the list of scores to probability, amplifying the probability of larger scores.

#### 1.3 More details
In our model, $$\theta$$ represents all parameters and it serves to transform one-hot vectors $$w$$ of all words in the dictionary into word vectors. With d-dimensional vector and V many words:

$$
\theta = 
\left[
 \begin{matrix}
   v_1\\
   v_2\\
   ...\\
   v_n\\
   u_1\\
   u_2\\
   ...\\
   u_n\\
 \end{matrix}
\right]
\in R^{2V*d}
$$

Or, we can use two matrices $$\theta_{center}$$ and $$\theta_{context}$$ to seperate center vector and context vector: 

$$
\theta = 
\left[
 \begin{matrix}
   \theta_{center}\\
   \theta_{context}\\
 \end{matrix}
\right]
\in R^{2V*d}
$$

$$
\theta_{center} = 
\left[
 \begin{matrix}
   v_1\\
   v_2\\
   ...\\
   v_n\\
 \end{matrix}
\right]
\in R^{V*d}

\qquad \theta_{context} = 
\left[
 \begin{matrix}
   u_1\\
   u_2\\
   ...\\
   u_n\\
 \end{matrix}
\right]
\in R^{V*d}
$$

Thus, the pipeline of Skip-Gram model can be expressed as:
$$
\theta_{center} w_t =
\left[
 \begin{matrix}
   v_1\\
   ...\\
   v_c\\
   ...\\
   v_n\\
 \end{matrix}
\right]
\left[
 \begin{matrix}
   0\\
   ...\\
   1\\
   ...\\
   0\\
 \end{matrix}
\right]
= v_c = 
\left[
 \begin{matrix}
   1.3\\
   ...\\
   -4.3\\
   ...\\
   3\\
 \end{matrix}
\right]
 \in R^{d*1}
$$

$$
\theta_{context}^{T} v_c =
\left[
 \begin{matrix}
   0.7\\
   ...\\
   0.3\\
   ...\\
   1.4\\
 \end{matrix}
\right]
= scores \in R^{V*1}
$$

$$
softmax(scores)=
\left[
 \begin{matrix}
   0.15\\
   ...\\
   0.08\\
   ...\\
   0.7\\
 \end{matrix}
\right]
= P(w_{t-m}|w_t, \theta)
= probability \in R^{V*1}
$$
Lastly the probability is used to calculate losss.