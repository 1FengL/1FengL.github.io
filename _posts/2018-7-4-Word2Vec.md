---
layout: post
title: Word2Vec
---

In NLP, words are the main objects that we deal with. How should we represent words in a form that computers can store? A straightforward answer can be representing them as discrete symbols:
<center>dog = [0 0 0 0 1 0 0 ... 0 0]</center>

<center>cat = [0 0 1 0 0 0 0 ... 0 0]</center>

which are also called one-hot vectors, where an 1 is in a location unique to the specific word and 0s are elsewhere.

However, one-hot representations have mainly two drawbacks:
* Vectors can be too long that we cannot afford as the dictionary size grows
+ All vectors are orthogonal and thus there is no natural notion of **similarity** in one-hot vectors.

Therefore, we would like to represent words in dense vectors that encode similarity to solve above drawbacks. Word vectors, also known as word embeddings or word representations, builds a dense vecotor for each word, and enables that similar meanings also have similar vectors. To do so, we may **represent a word according to words that frequently appear nearby**, for that synonyms can substitue each other and be placed in same context.

## Skip-Gram Model

The Skip-Gram model is a framework for learning word vectors. Under this framework, we represent each word by a vector in a way that, given a word, we can predict its surrounding words. To achieve this, the probability:

$$P(w_{t-m}|w_t)$$

is maximized, where \\(w_{t}\\) is the word vector of the center word and m is the window size. Specifically, we use a small window with size m to scan the whole sentence, and keep maximizing the probability.

![_config.yml]({{ site.baseurl }}/images/word2vec1.png)
<center>Fig.1</center>
![_config.yml]({{ site.baseurl }}/images/word2vec2.png)
<center>Fig.2</center>

Mathematically, we maximize the likelihood:

$$\max L(\theta) = \prod_{t=1}^{T}\prod_{-m\le j\le m, j\neq0}P(w_{t+j}|w_t,\theta)$$

where \\(\theta\\) is the parameters of Word2vec model.

We can put a log and a minus sign on both sides, to turn product into sum, and turn maximization into minimization, and then take average:

$$\min J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T} \sum_{t=1}^{T}\sum_{-m\le j\le m, j\neq0}logP(w_{t+j}|w_t,\theta)$$

Now, the question becomes how do we calculate probability $$ P(w_{t+j}|w_t,\theta) $$ ? Instead of giving each word one vector, we assign two different vecotrs to every word, one representing the word when it is a center word and the other when it is a context word:

* \\(v_w\\) when \\(w\\) is a center word
* \\(v_w\\) when \\(w\\) is a context word

Then for a center word \\(c\\) and a context word \\(o\\), the probability becomes:

$$P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\subseteq V} exp(u^T_w v_c))}$$

This probability is actually a **softmax** function,