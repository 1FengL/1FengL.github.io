---
layout: post
title: Word2Vec
---

In NLP, words are the main objects that we deal with. How should we represent words in a form that computers can store? A straightforward answer can be representing them as discrete symbols:
<center>dog = [0 0 0 0 1 0 0 ... 0 0]</center>

<center>cat = [0 0 1 0 0 0 0 ... 0 0]</center>

which are also called one-hot vectors, where an 1 is in a location unique to the specific word and 0s are elsewhere.

However, one-hot representations have mainly two drawbacks:
* Vectors can be too long that we cannot afford as the dictionary size grows
+ All vectors are orthogonal and thus there is no natural notion of **similarity** in one-hot vectors.

Therefore, we would like to represent words in dense vectors that encode similarity to solve above drawbacks. Word vectors, also known as word embeddings or word representations, builds a dense vecotor for each word, and enables that similar meanings also have similar vectors. To do so, we may **represent a word according to words that frequently appear nearby**, for that synonyms can substitue each other and be placed in same context.

##Skip-Gram Model

Word2vec is a framework for learning word vectors. Under this framework, we represent each word by a vector in a way that, given a word, we can predict its surrounding words. To achieve this target, the probability:

$$P(w_{t-m}|w_t)$$

is maximized, where \\(w_{t}\\) is the word vector of the center word and m is the window size. Specifically, we use a small window with size m to scan the whole sentence, and keep maximizing the probability.

![_config.yml]({{ site.baseurl }}/images/word2vec1.png)

![_config.yml]({{ site.baseurl }}/images/word2vec2.png)

Mathematically, we maximize the likelihood:

$$\max L(\theta) = \prod_{t=1}^{T}\prod_{-m\le j\le m, j\neq0}P(w_{t+j}|w_t,\theta)$$

where \\(\theta\\) is the parameters of Word2vec model.

We can put a log and a minus sign on both sides, to turn product into sum, and turn maximization into minimization, and then take average:

$$\min J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T} \sum_{t=1}^{T}\sum_{-m\le j\le m, j\neq0}logP(w_{t+j}|w_t,\theta)$$

Now, the question becomes how do we calculate probability \\(w_{t+j}|w_t,\theta\\)? 
