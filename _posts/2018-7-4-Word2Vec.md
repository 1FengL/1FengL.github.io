---
layout: post
title: Word2Vec
---

In NLP, words are the main objects that we deal with. How should we represent words in a form that computers can store? A straightforward answer can be representing them as discrete symbols:
<center>dog = [0 0 0 0 1 0 0 ... 0 0]</center>

<center>cat = [0 0 1 0 0 0 0 ... 0 0]</center>

which are also called one-hot vectors, where an 1 is in a location unique to the specific word and 0s are elsewhere.

However, one-hot representations have mainly two drawbacks:
* Vectors can be too long that we cannot afford as the dictionary size grows
+ All vectors are orthogonal and thus there is no natural notion of **similarity** in one-hot vectors.

Therefore, we would like to represent words in dense vectors that encode similarity to solve above drawbacks. For instance, we may want our word vectors end like:

$$
dog = 
 \left[
 \begin{matrix}
   1.1\\
   1.9\\
   ...\\
   3.9\\
 \end{matrix}
 \right]
cat = 
\left[
 \begin{matrix}
   1.2\\
   2.0\\
   ...\\
   4.5\\
 \end{matrix}
\right]
$$

Due to that dogs&cats are both animals, they have some dimensions sharing similar values. On the other hand, they belong to different species so some other dimensions vary in terms of value.

Word vectors, also known as word2vec, word embeddings or word representations, builds a dense vecotor for each word, and enables that similar meanings also have similar vectors. To do so, we may **represent a word according to words that frequently appear nearby**, for that synonyms can substitue each other and be placed in same context.

## 1. Skip-Gram Model

#### 1.1 Introduction

The Skip-Gram model is one of the frameworks for learning word vectors. Under this framework, we represent each word by a vector in a way that, given a word, we can predict its surrounding words. To achieve this, the probability:

$$P(w_{t-m}|w_t)$$

is maximized, where \\(w_{t}\\) is the word vector of the center word and m is the window size. Specifically, we use a small window with size m to scan the whole sentence, and keep maximizing the probability.

![_config.yml]({{ site.baseurl }}/images/word2vec1.png)
<center>Fig.1</center>
![_config.yml]({{ site.baseurl }}/images/word2vec2.png)
<center>Fig.2</center>

#### 1.2 Loss function

Mathematically, we maximize the likelihood:

$$\max L(\theta) = \prod_{t=1}^{T}\prod_{-m\le j\le m, j\neq0}P(w_{t+j}|w_t,\theta)$$

where \\(\theta\\) is the parameters of Word2vec model.

We can put a log and a minus sign on both sides, to turn product into sum, and turn maximization into minimization, and then take average:

$$\min J(\theta) = -\frac{1}{T}logL(\theta) = -\frac{1}{T} \sum_{t=1}^{T}\sum_{-m\le j\le m, j\neq0}logP(w_{t+j}|w_t,\theta)$$

Now, the question becomes how do we calculate probability $$ P(w_{t+j}|w_t,\theta) $$ ?
Instead of giving each word one vector, we assign two different vecotrs to every word, one representing the word when it is a center word and the other when it is a context word:

* \\(v_w\\) when \\(w\\) is a center word
* \\(v_w\\) when \\(w\\) is a context word

>Constructing two vectors makes it easier to optimize them later. At then end these two vectors are averaged to produce the final vector.

Then for a center word \\(c\\) and a context word \\(o\\), the probability becomes:

$$P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\subseteq V} exp(u^T_w v_c))}$$

where V is the window.
<font size=3>
>1.First, the dot product of vector of center word $$v_c$$ and context word $$u_w$$ is calculated as a score, measuring how far these two vectors are.
>
>2.Then, a **softmax** function:
>$$ softmax(x_i) = \frac{exp(x_i)}{\sum_{j=1}^{n}{exp(x_j)}} = p_i $$
>is applied to calculate probability $$P(o|c)$$
>
>3.Softmax function maps the list of scores to probability, amplifying the probability of larger scores.
</font>

#### 1.3 Training the model
